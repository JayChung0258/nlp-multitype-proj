{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sanity Check\n",
        "\n",
        "**NLP Multi-Type Classification Project**\n",
        "\n",
        "This notebook performs sanity checks on a small subset of the data to verify:\n",
        "- Data loading works correctly\n",
        "- Label mappings are consistent\n",
        "- No obvious data quality issues\n",
        "- Configs resolve to real paths\n",
        "\n",
        "## Sanity Check Checklist\n",
        "\n",
        "### Data Loading\n",
        "- [ ] Load 1% subset of training data\n",
        "- [ ] Verify schema matches expected format\n",
        "- [ ] Print sample rows for manual inspection\n",
        "\n",
        "### Label Validation\n",
        "- [ ] Verify all labels are in expected range\n",
        "- [ ] Test label mapping T1/T2/T3/T4 ↔ 0/1/2/3\n",
        "- [ ] Ensure consistency across splits\n",
        "\n",
        "### Config Validation\n",
        "- [ ] Load all config files (data_config.yaml, models_baseline.yaml, etc.)\n",
        "- [ ] Verify all required keys exist\n",
        "- [ ] Verify paths resolve correctly\n",
        "\n",
        "### Split Integrity\n",
        "- [ ] Verify no family_id appears in multiple splits (smoke test)\n",
        "- [ ] Check label distribution is reasonable\n",
        "\n",
        "### Metrics Function Contracts\n",
        "- [ ] Verify eval_utils schemas are loaded\n",
        "- [ ] Verify result file schemas are defined\n",
        "\n",
        "---\n",
        "\n",
        "**TODO**: Implement code cells below to complete each checklist item.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append('../src')\n",
        "from constants import LABELS, LABEL2ID, ID2LABEL, REQUIRED_COLUMNS\n",
        "from schema import ProcessedRow\n",
        "\n",
        "print(\"Libraries imported successfully.\")\n",
        "print(f\"Labels: {LABELS}\")\n",
        "print(f\"Label mapping: {LABEL2ID}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Small Subset\n",
        "\n",
        "Load 1% of training data for quick validation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Load subset\n",
        "# train_df = pd.read_csv('../data/processed/train_4class.csv')\n",
        "# subset_df = train_df.sample(frac=0.01, random_state=42)\n",
        "# print(f\"Loaded {len(subset_df)} samples (1% of train)\")\n",
        "# print(subset_df.head())\n",
        "\n",
        "print(\"TODO: Implement subset loading\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Verify Label Mapping\n",
        "\n",
        "Test that label mappings are consistent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Verify label mapping\n",
        "# print(\"Label distribution in subset:\")\n",
        "# print(subset_df['label'].value_counts().sort_index())\n",
        "\n",
        "# # Test mapping\n",
        "# for label_str, label_int in LABEL2ID.items():\n",
        "#     reverse = ID2LABEL[label_int]\n",
        "#     assert reverse == label_str, f\"Mapping inconsistency: {label_str} -> {label_int} -> {reverse}\"\n",
        "#     print(f\"✓ {label_str} ↔ {label_int}\")\n",
        "\n",
        "print(\"TODO: Implement label validation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load and Validate Configs\n",
        "\n",
        "Load all YAML configs and verify structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Load configs\n",
        "# config_files = [\n",
        "#     '../configs/data_config.yaml',\n",
        "#     '../configs/models_baseline.yaml',\n",
        "#     '../configs/models_transformer.yaml',\n",
        "#     '../configs/project.yaml'\n",
        "# ]\n",
        "\n",
        "# for config_file in config_files:\n",
        "#     with open(config_file, 'r') as f:\n",
        "#         config = yaml.safe_load(f)\n",
        "#     print(f\"✓ Loaded {Path(config_file).name}\")\n",
        "#     print(f\"  Keys: {list(config.keys())}\")\n",
        "\n",
        "print(\"TODO: Implement config loading\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Verify No Cross-Split Families\n",
        "\n",
        "Quick check on subset to ensure family grouping works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Verify family grouping\n",
        "# print(f\"Unique families in subset: {subset_df['family_id'].nunique()}\")\n",
        "# print(\"Family_id value counts (top 10):\")\n",
        "# print(subset_df['family_id'].value_counts().head(10))\n",
        "\n",
        "print(\"TODO: Implement family grouping check\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
