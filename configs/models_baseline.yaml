# Baseline Model Configuration for NLP Multi-Type Classification Project
# This file defines classical ML models and their hyperparameters.

# ============================================================
# TF-IDF FEATURE EXTRACTION
# ============================================================
tfidf:
  # N-gram range for TF-IDF vectorization
  ngram_range: [1, 2]  # unigrams and bigrams
  
  # Maximum number of features to extract
  max_features: 10000
  
  # Convert text to lowercase before tokenization
  lowercase: true
  
  # Additional TF-IDF parameters (can be extended)
  # min_df: 2
  # max_df: 0.95
  # sublinear_tf: true

# ============================================================
# BASELINE MODELS
# ============================================================
# Each model entry defines name and hyperparameters
# These will be used with TF-IDF features

models:
  # ------------------------------
  # Logistic Regression
  # ------------------------------
  - name: logreg
    # Handle class imbalance
    class_weight: balanced
    # Maximum iterations for convergence
    max_iter: 2000
    # Solver for optimization
    solver: lbfgs
    # Multi-class strategy
    multi_class: multinomial
    # Random seed
    random_state: 42
  
  # ------------------------------
  # Linear SVM
  # ------------------------------
  - name: linear_svm
    # Handle class imbalance
    class_weight: balanced
    # Maximum iterations
    max_iter: 5000
    # Regularization parameter (can tune)
    C: 1.0
    # Loss function
    loss: hinge
    # Random seed
    random_state: 42
  
  # ------------------------------
  # Random Forest
  # ------------------------------
  - name: random_forest
    # Number of trees
    n_estimators: 500
    # Maximum tree depth (null = no limit)
    max_depth: null
    # Minimum samples to split a node
    min_samples_split: 2
    # Minimum samples per leaf
    min_samples_leaf: 1
    # Handle class imbalance
    class_weight: balanced
    # Random seed
    random_state: 42
    # Use all available cores
    n_jobs: -1
  
  # ------------------------------
  # XGBoost
  # ------------------------------
  - name: xgboost
    # Number of boosting rounds
    n_estimators: 800
    # Learning rate
    learning_rate: 0.1
    # Maximum tree depth
    max_depth: 6
    # L2 regularization
    reg_lambda: 1.0
    # L1 regularization
    reg_alpha: 0.0
    # Subsample ratio
    subsample: 0.8
    # Feature subsample ratio per tree
    colsample_bytree: 0.8
    # Objective function for multi-class
    objective: multi:softmax
    # Evaluation metric
    eval_metric: mlogloss
    # Random seed
    random_state: 42
    # Use all available cores
    n_jobs: -1

# ============================================================
# TRAINING CONFIGURATION
# ============================================================
training:
  # Cross-validation folds (for hyperparameter tuning, if needed)
  cv_folds: 5
  
  # Verbose output during training
  verbose: 1
  
  # Save trained models
  save_models: true
  
  # Model output directory
  model_output_dir: results/baseline/models/

