# Data Directory

This directory contains raw and processed data for the NLP multi-type classification project.

---

## Directory Structure

```
data/
‚îú‚îÄ‚îÄ raw/                      # Original, unprocessed data files
‚îÇ   ‚îî‚îÄ‚îÄ (place your raw data here)
‚îÇ
‚îî‚îÄ‚îÄ processed/                # Generated by data_prep.py
    ‚îú‚îÄ‚îÄ train_4class.jsonl    # Training split (70%) - JSONL format
    ‚îú‚îÄ‚îÄ val_4class.jsonl      # Validation split (15%) - JSONL format
    ‚îú‚îÄ‚îÄ test_4class.jsonl     # Test split (15%) - JSONL format
    ‚îî‚îÄ‚îÄ manifest.json         # Dataset statistics and metadata
```

---

## Raw Data

### Where to Place Raw Data

Place your source data files in the `raw/` subdirectory:

```bash
cp /path/to/your/dataset.json data/raw/
# or
cp /path/to/your/dataset.csv data/raw/
```

### Expected Raw Data Format

Raw data should be organized by **families**, where each family represents variants of the same base content.

#### Required Fields

| Field | Type | Description |
|-------|------|-------------|
| `family_id` | string | Unique identifier for grouping samples |
| `type1` | string or null | Human original text |
| `type2` | string or null | LLM generated text |
| `type3` | string or null | Human paraphrased text |
| `type4` | string or null | LLM paraphrased text |

#### Example (JSON)

```json
[
  {
    "idx": 0,
    "dataset_source": "mrpc",
    "human_original_text(type1)": "The weather today is sunny and warm.",
    "llm_generated_text(type2)": "Today's weather is characterized by sunshine and warmth.",
    "human_paraphrased_text(type3)": "It's a sunny, warm day outside.",
    "llm_paraphrased_original_text(type4)-prompt-based": "The current meteorological conditions feature abundant sunshine."
  },
  {
    "idx": 1,
    "dataset_source": "paws",
    "human_original_text(type1)": "Machine learning is a subset of artificial intelligence.",
    "llm_generated_text(type2)": "ML represents a branch of AI technology.",
    "human_paraphrased_text(type3)": "Artificial intelligence includes machine learning as a component.",
    "llm_paraphrased_original_text(type4)-prompt-based": null
  }
]
```

**Note:** Not all families need to have all four types. Missing types (null values) are acceptable.

**Family ID Construction:** The preprocessing script constructs `family_id` as `"<dataset_source>_<idx>"` (e.g., `"mrpc_0"`, `"paws_42"`).

---

## Processed Data

### How Processed Data is Generated

Processed data is created by running:

```bash
python3 -m src.data_prep
```

This script:
1. Reads raw JSON/JSONL data from `data/raw/`
2. Normalizes text (Unicode NFKC, whitespace cleanup)
3. Materializes individual rows for each type (T1-T4)
4. Performs family-aware splitting (70% train, 15% val, 15% test)
5. Deduplicates within splits
6. Validates data integrity (no leakage, valid labels, etc.)
7. Writes JSONL files to `data/processed/`
8. Generates manifest with statistics

### Processed Data Format (JSONL)

Each JSONL file contains individual classification samples with one JSON object per line.

| Field | Type | Description |
|-------|------|-------------|
| `id` | string | Globally unique row identifier: `"<family_id>__<label>"` |
| `family_id` | string | Links back to original family (format: `"<source>_<idx>"`) |
| `source` | string | Data source: "mrpc", "paws", or "hlpc" |
| `text` | string | The sentence to classify (normalized) |
| `label` | string | One of {T1, T2, T3, T4} |
| `label_id` | integer | Numeric label: 0, 1, 2, or 3 |
| `text_len_char` | integer | Character count |
| `text_len_word` | integer | Word count (whitespace split) |

#### Example (train_4class.jsonl)

```jsonl
{"id": "mrpc_0__T1", "family_id": "mrpc_0", "source": "mrpc", "text": "The weather today is sunny and warm.", "label": "T1", "label_id": 0, "text_len_char": 39, "text_len_word": 7}
{"id": "mrpc_0__T2", "family_id": "mrpc_0", "source": "mrpc", "text": "Today's weather is characterized by sunshine and warmth.", "label": "T2", "label_id": 1, "text_len_char": 58, "text_len_word": 8}
{"id": "mrpc_0__T3", "family_id": "mrpc_0", "source": "mrpc", "text": "It's a sunny, warm day outside.", "label": "T3", "label_id": 2, "text_len_char": 33, "text_len_word": 6}
{"id": "paws_1__T1", "family_id": "paws_1", "source": "paws", "text": "Machine learning is a subset of artificial intelligence.", "label": "T1", "label_id": 0, "text_len_char": 58, "text_len_word": 9}
```

**Critical:** All samples with the same `family_id` reside in the **same split** (no leakage).

#### Manifest File (manifest.json)

Contains dataset statistics:
- Row counts per split
- Label distribution per split
- Average text lengths
- Dropped row counts
- SHA1 hash of family IDs (for verification)

---

## Label Mapping

| String Label | Integer | Description |
|--------------|---------|-------------|
| **T1** | 0 | Human Original |
| **T2** | 1 | LLM Generated |
| **T3** | 2 | Human Paraphrased |
| **T4** | 3 | LLM Paraphrased |

---

## Data Validation

### Validation Checks Performed

The data preparation script validates:

‚úÖ **Label validity:** All labels in {T1, T2, T3, T4}  
‚úÖ **Non-empty text:** All texts are non-empty after stripping whitespace  
‚úÖ **No leakage:** Each `family_id` appears in exactly one split  
‚úÖ **Text length:** All texts ‚â§ 4000 characters (configurable)  

### Warning Checks

‚ö†Ô∏è **Class imbalance:** Warn if any class < 10% of total  
‚ö†Ô∏è **Text length outliers:** Warn if text < 10 or > 2000 characters  
‚ö†Ô∏è **Duplicates:** Count exact duplicate texts  

---

## Data Statistics (Example)

After running data preparation, you should see output like:

```
Train split: 2660 samples (700 families)
Val split:   570 samples (150 families)
Test split:  570 samples (150 families)

Class distribution (train):
  T1: 720 (27.1%)
  T2: 680 (25.6%)
  T3: 310 (11.7%)
  T4: 950 (35.7%)

Text length statistics (train):
  Mean: 87.3 characters (14.2 words)
  Median: 76 characters (12 words)
  Min: 15 characters (3 words)
  Max: 312 characters (48 words)
```

---

## Split Determinism

### Fixed Seed for Reproducibility

The split uses a **fixed random seed** (default: 42) to ensure:
- Same splits generated across multiple runs
- Reproducible experiments
- Consistent results

To change the seed, edit `configs/data_config.yaml`:

```yaml
split:
  seed: 42  # Change this to generate different splits
```

**Warning:** Changing the seed will produce different train/val/test splits.

---

## Data Quality Best Practices

### Before Running Data Preparation

1. **Inspect raw data:**
   - Check for missing `family_id` values
   - Verify text fields are not empty
   - Ensure at least some families have all 4 types

2. **Check for duplicates:**
   - Look for duplicate `family_id` values
   - Identify duplicate text content

3. **Validate encoding:**
   - Ensure files are UTF-8 encoded
   - Check for special characters or control characters

### After Running Data Preparation

1. **Run EDA notebook:**
   ```bash
   jupyter notebook notebooks/00_eda.ipynb
   ```

2. **Verify leakage check passed:**
   - Look for "‚úì PASS: No family leakage detected" in output

3. **Review class distribution:**
   - Ensure all classes are represented
   - Check if imbalance is acceptable

4. **Inspect text lengths:**
   - Verify no extremely short or long texts
   - Review outliers manually

---

## Troubleshooting

### Issue: "No family leakage check failed"

**Cause:** Some `family_id` appears in multiple splits.

**Solution:**
- Check raw data for duplicate `family_id` values
- Verify `group_key: family_id` in `configs/data_config.yaml`
- Run with a different seed or fix data quality issues

### Issue: "Class T3 is X% (below 10% threshold)"

**Cause:** Class T3 is underrepresented in the dataset.

**Solution:**
- If T3 is genuinely rare, this is expected (just a warning)
- Consider collecting more T3 samples
- Use class weighting during training to compensate

### Issue: "Maximum text length exceeded"

**Cause:** Some texts are longer than `max_len_chars` (default: 4000).

**Solution:**
- Review the long texts manually (are they outliers?)
- Increase `max_len_chars` in `configs/data_config.yaml` if needed
- Or truncate/remove very long texts

---

## File Naming Conventions

- **Raw files:** Any name is acceptable (e.g., `data.json`, `dataset.csv`)
- **Processed files:** Fixed names (do not rename):
  - `train_4class.csv`
  - `val_4class.csv`
  - `test_4class.csv`

**Rationale:** Training scripts expect these exact file names.

---

## Backup and Version Control

### What to Version Control

‚úÖ **Raw data:** If small enough, commit to Git  
‚úÖ **Data preparation script:** `src/data_prep.py`  
‚úÖ **Config files:** `configs/data_config.yaml`  

### What NOT to Version Control

‚ùå **Processed data:** Large CSVs (regenerate as needed)  
‚ùå **Intermediate files:** Temporary processing artifacts  

**Recommendation:** Use `.gitignore` to exclude `data/processed/`:

```
# .gitignore
data/processed/
```

---

## Data Contract

For complete schema specifications, validation rules, and data contracts, see:

üìÑ [`docs/DATA_CONTRACT.md`](../docs/DATA_CONTRACT.md)

---

*Last updated: 2025-11-13*

