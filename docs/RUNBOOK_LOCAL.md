# Local Runbook

**NLP Multi-Type Classification Project**

This runbook provides step-by-step instructions for running the project on your local machine.

---

## Table of Contents

1. [Prerequisites](#prerequisites)
2. [Initial Setup](#initial-setup)
3. [Data Preparation](#data-preparation)
4. [Exploratory Data Analysis](#exploratory-data-analysis)
5. [Training Models (Future)](#training-models-future)
6. [Evaluation and Results](#evaluation-and-results)
7. [Troubleshooting](#troubleshooting)
8. [Common Pitfalls](#common-pitfalls)

---

## Prerequisites

### System Requirements

- **Operating System:** Linux, macOS, or Windows with WSL
- **Python:** 3.8 or higher
- **RAM:** At least 8GB (16GB recommended for transformer models)
- **Storage:** At least 5GB free space
- **GPU:** Optional but recommended for transformer training (CUDA-compatible)

### Software Dependencies

- Python 3.8+
- pip or conda package manager
- Git
- (Optional) CUDA toolkit if using GPU

---

## Initial Setup

### Step 1: Clone Repository

```bash
git clone <repository-url>
cd nlp-multitype-proj
```

### Step 2: Create Virtual Environment

**Using venv:**

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

**Using conda:**

```bash
conda create -n nlp-multitype python=3.9
conda activate nlp-multitype
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

**Verify installation:**

```bash
python -c "import pandas, sklearn, transformers; print('Dependencies installed successfully!')"
```

### Step 4: Verify Directory Structure

```bash
ls -R
```

Expected structure:

```
nlp-multitype-proj/
├── data/
│   ├── raw/              (place your raw data here)
│   └── processed/        (generated by data_prep)
├── src/
├── notebooks/
├── configs/
├── results/
├── figures/
├── docs/
└── requirements.txt
```

---

## Data Preparation

### Step 1: Place Raw Data

Copy your raw data files into `data/raw/`:

```bash
cp /path/to/your/raw_data.json data/raw/
# or
cp /path/to/your/raw_data.csv data/raw/
```

**Expected format:** See `DATA_CONTRACT.md` for schema details.

### Step 2: Configure Data Processing

Edit `configs/data_config.yaml` if needed:

```yaml
raw_dir: data/raw
processed_dir: data/processed
split:
  train_ratio: 0.70
  val_ratio: 0.15
  test_ratio: 0.15
  seed: 42
  group_key: family_id
```

**Key settings:**
- `seed`: Change this to generate different random splits
- `group_key`: Must be `family_id` for family-aware splitting
- `max_len_chars`: Adjust based on your data

### Step 3: Run Data Preparation

```bash
python3 -m src.data_prep
```

**Note:** The script reads configuration from `configs/data_config.yaml` automatically.

**Expected output:**

```
============================================================
NLP Multi-Type Classification: Data Preparation
============================================================

Configuration:
  Raw data directory: data/raw
  Processed data directory: data/processed
  Split ratios: {'train': 0.7, 'val': 0.15, 'test': 0.15}
  Random seed: 42

[1/7] Loading raw data from data/raw
Found 2 raw data file(s)
  Loading: data.json
Loaded 10000 families from raw data

[2/7] Materializing rows from families
Materialized 35420 rows from families

[3/7] Performing family-aware split
Splitting 10000 families into train/val/test
  Train families: 7000
  Val families: 1500
  Test families: 1500
  Train rows: 24794
  Val rows: 5313
  Test rows: 5313

[4/7] Deduplicating within splits
  Train split:
    Removed 23 duplicate rows
  Val split:
  Test split:

[5/7] Validating splits
  ✓ PASS: No family leakage detected
  ✓ PASS: All rows have valid field values
  ✓ Train split has all labels
  ✓ Val split has all labels
  ✓ Test split has all labels

[6/7] Writing JSONL files to data/processed
  Wrote 24771 rows to train_4class.jsonl
  Wrote 5313 rows to val_4class.jsonl
  Wrote 5313 rows to test_4class.jsonl

[7/7] Computing and writing manifest
  Wrote manifest to manifest.json

============================================================
SUMMARY
============================================================
Total rows: 35397
Total families: 10000

Rows per split:
  train: 24771
  val: 5313
  test: 5313

Rows per label per split:
  train:
    T1: 6850
    T2: 6723
    T3: 5601
    T4: 5597
  val:
    T1: 1469
    T2: 1441
    T3: 1201
    T4: 1202
  test:
    T1: 1469
    T2: 1441
    T3: 1201
    T4: 1202

Dropped rows:
  empty_text: 23
  too_long: 0

Data preparation complete!
============================================================
```

### Step 4: Verify Output Files

```bash
ls -lh data/processed/
```

Expected files:
- `train_4class.jsonl` — Training split (JSONL format)
- `val_4class.jsonl` — Validation split (JSONL format)
- `test_4class.jsonl` — Test split (JSONL format)
- `manifest.json` — Dataset statistics and metadata

**Quick check:**

```bash
# Count lines in JSONL files
wc -l data/processed/*.jsonl

# View first few rows
head -n 3 data/processed/train_4class.jsonl

# View manifest
cat data/processed/manifest.json | python3 -m json.tool
```

---

## Exploratory Data Analysis

### Step 1: Open EDA Notebook

```bash
jupyter notebook notebooks/00_eda.ipynb
# or
jupyter lab notebooks/00_eda.ipynb
```

### Step 2: Run All Cells

Execute all cells in order. The notebook will:

1. Load processed CSVs
2. Display class distributions
3. Analyze text length distributions
4. Check for duplicates
5. Verify no family leakage
6. Generate visualizations

**Key outputs:**
- Class distribution bar charts → `figures/data/class_distribution.png`
- Text length boxplots → `figures/data/text_length_by_class.png`
- Summary statistics printed in notebook

### Step 3: Sanity Check

Open and run `notebooks/01_sanity_check.ipynb`:

```bash
jupyter notebook notebooks/01_sanity_check.ipynb
```

This notebook:
- Loads a small subset (1% of train)
- Verifies label mappings are consistent
- Validates config files load correctly
- Confirms no obvious data issues

**Expected result:** All checks pass with ✓ markers.

---

## Training Models (Future)

**Note:** Training scripts are not yet implemented in this phase. This section describes the planned workflow.

### Baseline Models (TF-IDF + Classical ML)

**Planned command:**

```bash
python src/train_baseline.py \
    --data-config configs/data_config.yaml \
    --model-config configs/models_baseline.yaml \
    --output-dir results/baseline/
```

**Expected behavior:**
1. Load train/val/test CSVs
2. Extract TF-IDF features
3. Train each model (Logistic Regression, SVM, Random Forest, XGBoost)
4. Evaluate on validation set
5. Report metrics and save results

### Transformer Models (Fine-tuning)

**Planned command:**

```bash
python src/train_transformer.py \
    --data-config configs/data_config.yaml \
    --model-config configs/models_transformer.yaml \
    --model-name bert-base-uncased \
    --output-dir results/transformer/
```

**Expected behavior:**
1. Load train/val/test CSVs
2. Tokenize texts using model-specific tokenizer
3. Fine-tune transformer for 3 epochs
4. Evaluate on validation set
5. Save checkpoint and metrics

---

## Evaluation and Results

### Results Directory Structure

After training (future), results will be organized as:

```
results/
├── baseline/
│   ├── logreg_metrics.json
│   ├── linear_svm_metrics.json
│   ├── random_forest_metrics.json
│   ├── xgboost_metrics.json
│   └── baseline_summary.csv
├── transformer/
│   ├── bert-base-uncased_metrics.json
│   ├── roberta-base_metrics.json
│   ├── deberta-v3-base_metrics.json
│   └── transformer_summary.csv
└── robustness/
    └── perturbation_results.csv
```

### Metrics Format

**Per-run JSON (`model_name_metrics.json`):**

```json
{
  "model_name": "bert-base-uncased",
  "accuracy": 0.8523,
  "macro_f1": 0.8341,
  "per_class_f1": {
    "T1": 0.8912,
    "T2": 0.8654,
    "T3": 0.7523,
    "T4": 0.8275
  },
  "train_time_sec": 1234.5,
  "inference_latency_ms_per_sample": 12.3,
  "num_parameters": 110000000,
  "seed": 42,
  "timestamp_utc": "2025-11-13T10:30:00Z"
}
```

**Aggregated CSV (`summary.csv`):**

| model_name | accuracy | macro_f1 | f1_T1 | f1_T2 | f1_T3 | f1_T4 | train_time_sec | latency_ms | num_params |
|------------|----------|----------|-------|-------|-------|-------|----------------|------------|------------|
| logreg | 0.7834 | 0.7621 | 0.8123 | 0.7856 | 0.6934 | 0.7571 | 12.3 | 0.8 | 40000 |
| bert-base | 0.8523 | 0.8341 | 0.8912 | 0.8654 | 0.7523 | 0.8275 | 1234.5 | 12.3 | 110000000 |

---

## Troubleshooting

### Issue: `ModuleNotFoundError: No module named 'src'`

**Solution:** Ensure you're running scripts from the project root:

```bash
cd /path/to/nlp-multitype-proj
python src/data_prep.py ...
```

Or add `src/` to Python path:

```bash
export PYTHONPATH="${PYTHONPATH}:$(pwd)/src"
```

### Issue: Data preparation fails with "No family leakage check failed"

**Cause:** Some `family_id` appears in multiple splits.

**Solution:**
1. Check if raw data has duplicate `family_id` values with conflicting content
2. Verify `group_key` in config is set to `family_id`
3. Re-run with a different seed or fix data quality issues

### Issue: Class imbalance warning

**Symptom:** Warning message: "Class T3 is X% (below 10% threshold)"

**Solution:**
- If T3 is genuinely rare in your data, this is expected
- Consider stratified sampling or class weighting during training
- Check if raw data is missing T3 samples

### Issue: Out of memory during transformer training

**Solution:**
- Reduce `per_device_train_batch_size` in `configs/models_transformer.yaml`
- Reduce `max_seq_length` (e.g., from 256 to 128)
- Use a smaller model (e.g., DistilBERT instead of BERT)
- Enable gradient accumulation to simulate larger batches

### Issue: Jupyter kernel not found

**Solution:**

```bash
python -m ipykernel install --user --name nlp-multitype --display-name "Python (NLP Multi-Type)"
```

Then select the kernel in Jupyter.

---

## Common Pitfalls

### Pitfall 1: Not Using Family-Aware Splitting

**Problem:** Splitting randomly without grouping by `family_id` causes data leakage.

**Consequence:** Artificially inflated performance; model memorizes content.

**Prevention:**
- Always use `group_key: family_id` in `data_config.yaml`
- Verify leakage check passes in data preparation output
- Run EDA notebook cell that checks for cross-split families

### Pitfall 2: Inconsistent Label Mapping

**Problem:** Using different label mappings in different parts of the code.

**Consequence:** Mismatched predictions, incorrect metrics.

**Prevention:**
- Always import from `constants.py`: `from constants import LABEL2ID, ID2LABEL`
- Never hardcode label mappings
- Verify label consistency in sanity check notebook

### Pitfall 3: Forgetting to Deduplicate Before Splitting

**Problem:** Duplicate texts end up in multiple splits.

**Consequence:** Data leakage, inflated performance.

**Prevention:**
- Data preparation script handles this automatically
- Verify "Removed N duplicate texts" in data prep output
- Check duplicate count in EDA notebook

### Pitfall 4: Not Setting Random Seed

**Problem:** Different splits generated each run.

**Consequence:** Non-reproducible results.

**Prevention:**
- Always set `seed: 42` in `data_config.yaml`
- Document seed in all experiment logs
- Use same seed for model training as well

### Pitfall 5: Ignoring Text Length Outliers

**Problem:** Very long or very short texts can cause issues.

**Consequence:** Transformer truncation, poor model performance on outliers.

**Prevention:**
- Review text length distribution in EDA
- Set appropriate `max_len_chars` in config
- Consider filtering or truncating extreme outliers

---

## Quick Reference Commands

### Data Preparation

```bash
python src/data_prep.py --config configs/data_config.yaml
```

### EDA

```bash
jupyter notebook notebooks/00_eda.ipynb
```

### Sanity Check

```bash
jupyter notebook notebooks/01_sanity_check.ipynb
```

### View Logs

```bash
tail -f logs/data_prep.log  # (if logging is implemented)
```

### Count Samples

```bash
wc -l data/processed/*.csv
```

### Check for Leakage Manually

```bash
python -c "
import pandas as pd
train = pd.read_csv('data/processed/train_4class.csv')
val = pd.read_csv('data/processed/val_4class.csv')
test = pd.read_csv('data/processed/test_4class.csv')
train_fams = set(train['family_id'])
val_fams = set(val['family_id'])
test_fams = set(test['family_id'])
overlap = train_fams & val_fams | train_fams & test_fams | val_fams & test_fams
print(f'Overlap count: {len(overlap)}')
print('PASS' if len(overlap) == 0 else 'FAIL')
"
```

---

## Next Steps

Once data preparation and EDA are complete:

1. Implement training scripts (`src/train_baseline.py`, `src/train_transformer.py`)
2. Run baseline models to establish performance floor
3. Run transformer models for state-of-the-art comparison
4. Analyze results, generate plots, compare models
5. Prepare for AWS migration (see `RUNBOOK_AWS.md`)

---

*Last updated: 2025-11-13*

